{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t09eeeR5prIJ"
      },
      "source": [
        "##### Copyright 2019 The TensorFlow Authors. (Traducción al español por Juan José Rojas Constaín X & Github: @jrconstain)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "GCCk8_dHpuNf"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ovpZyIhNIgoq"
      },
      "source": [
        "# Generación de texto con RNN (Redes Neuronales Recurrentes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hcD2nPQvPOFM"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://www.tensorflow.org/text/tutorials/text_generation\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/text/blob/master/docs/tutorials/text_generation.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/text/blob/master/docs/tutorials/text_generation.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://storage.googleapis.com/tensorflow_docs/text/docs/tutorials/text_generation.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BwpJ5IffzRG6"
      },
      "source": [
        "Este tutorial demuestra cómo generar texto usando una Red Neuronal Recurrente (RNN) basada en caracteres (letras, espacios, etc). Se trabajará con un conjunto de datos de los escritos de Shakespeare's tomados del artículo de Andrej Karpathy's [\"The Unreasonable Effectiveness of Recurrent Neural Networks\"](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) (La irrazonable efectividad de las Redes Neuronales Recurrentes). Dada una secuencia de caracteres de estos datos (p.e. \"Shakespear\"), se entrenará un modelo para predecir el próximo caracter en la secuencia (\"e\"). Cadenas de texto más largas pueden ser generadas ejecutando el modelo repetidamente.\n",
        "\n",
        "Nota: Habilite la aceleración de GPU para ejecutar más rápido este cuadero. En Colab: *Entorno de ejecución > Cambiar tipo de entorno de ejecución > Acelerador por hardware > GPU*.\n",
        "\n",
        "Este tutorial incluye código ejecutable implementado usando ['tf.keras'](https://www.tensorflow.org/guide/keras/sequential_model) y ['eager execution'](https://www.tensorflow.org/guide/eager). La siguiente es una muestra del resultado, cuando el modelo en este tutorial se entrenó durante 30 épocas y se inició con el indicador \"Q\":"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HcygKkEVZBaa"
      },
      "source": [
        "<pre>\n",
        "QUEENE:\n",
        "I had thought thou hadst a Roman; for the oracle,\n",
        "Thus by All bids the man against the word,\n",
        "Which are so weak of care, by old care done;\n",
        "Your children were in your holy love,\n",
        "And the precipitation through the bleeding throne.\n",
        "\n",
        "BISHOP OF ELY:\n",
        "Marry, and will, my lord, to weep in such a one were prettiest;\n",
        "Yet now I was adopted heir\n",
        "Of the world's lamentable day,\n",
        "To watch the next way with his father with his face?\n",
        "\n",
        "ESCALUS:\n",
        "The cause why then we are all resolved more sons.\n",
        "\n",
        "VOLUMNIA:\n",
        "O, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, it is no sin it should be dead,\n",
        "And love and pale as any will to that word.\n",
        "\n",
        "QUEEN ELIZABETH:\n",
        "But how long have I heard the soul for this world,\n",
        "And show his hands of life be proved to stand.\n",
        "\n",
        "PETRUCHIO:\n",
        "I say he look'd on, if I must be content\n",
        "To stay him from the fatal of our country's bliss.\n",
        "His lordship pluck'd from this sentence then for prey,\n",
        "And then let us twain, being the moon,\n",
        "were she such a case as fills m\n",
        "</pre>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_bGsCP9DZFQ5"
      },
      "source": [
        "While some of the sentences are grammatical, most do not make sense. The model has not learned the meaning of words, but consider:\n",
        "\n",
        "* The model is character-based. When training started, the model did not know how to spell an English word, or that words were even a unit of text.\n",
        "\n",
        "* The structure of the output resembles a play—blocks of text generally begin with a speaker name, in all capital letters similar to the dataset.\n",
        "\n",
        "* As demonstrated below, the model is trained on small batches of text (100 characters each), and is still able to generate a longer sequence of text with coherent structure."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srXC6pLGLwS6"
      },
      "source": [
        "## Configuración"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGyKZj3bzf9p"
      },
      "source": [
        "##Importar TensorFlow, otras librerías y datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "yG_n40gFzf9s"
      },
      "outputs": [],
      "source": [
        "# Se importa la biblioteca TensorFlow, que es una plataforma de código abierto para machine learning.\n",
        "import tensorflow as tf\n",
        "\n",
        "# Se importa la biblioteca numpy, que es una biblioteca para el manejo de arrays y matrices, así como\n",
        "# la realización de operaciones matemáticas en ellos. Es ampliamente utilizada en ciencia de datos y computación científica.\n",
        "import numpy as np\n",
        "\n",
        "# Se importa el módulo os, que provee una manera de usar funcionalidades dependientes del sistema operativo,\n",
        "# como leer o escribir en el sistema de archivos.\n",
        "import os\n",
        "\n",
        "# Se importa el módulo time, que provee funciones para trabajar con tiempos, como pausar la ejecución y medir el tiempo.\n",
        "import time\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHDoRoc5PKWz"
      },
      "source": [
        "### Descarga el conjunto de datos 'Shakespeare'\n",
        "\n",
        "Cambiar la siguiente línea para correr este código con datos propios."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "pD_55cOxLkAb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ebff270-2205-4986-f167-ade42ad0dee9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1115394/1115394 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "# Se define la variable 'path_to_file'.\n",
        "# La función 'get_file' de 'tf.keras.utils' se utiliza para descargar un archivo desde una URL y almacenarlo localmente.\n",
        "# Si el archivo ya ha sido descargado previamente, la función lo recuperará desde la caché en lugar de volver a descargarlo.\n",
        "# El primer argumento ('shakespeare.txt') es el nombre que se le dará al archivo descargado.\n",
        "# El segundo argumento es la URL desde donde se descargará el archivo.\n",
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHjdCjDuSvX_"
      },
      "source": [
        "### Lectura de datos\n",
        "\n",
        "Una mirada dentro del texto:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "aavnuByVymwK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5969e69-4448-4040-f937-514d54e101a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Longitud del texto: 1115394 caracteres\n"
          ]
        }
      ],
      "source": [
        "# Abre el archivo en ruta 'path_to_file' en modo binario ('rb') para lectura.\n",
        "# Lee el contenido completo del archivo con 'read()'.\n",
        "# Decodifica el contenido binario a cadenas de texto usando UTF-8 como codificación\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "\n",
        "# Imprime longitud del texto, que es el número total de caracteres en él.\n",
        "# Utiliza una f-string (cadena formateada) para insertar el valor de 'len(text)' dentro del mensaje.\n",
        "print(f'Longitud del texto: {len(text)} caracteres')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Duhg9NrUymwO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "255f4c27-2ef7-4aaf-8187-d08fb85eae1c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "All:\n",
            "We know't, we know't.\n",
            "\n",
            "First Citizen:\n",
            "Let us\n"
          ]
        }
      ],
      "source": [
        "# Imprimir los primeros 300 caracters en el texto\n",
        "print(text[:300])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "IlCgQBRVymwR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "480f94c9-a124-4455-9b6b-be7e8233be1e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "65 caracteres únicos\n"
          ]
        }
      ],
      "source": [
        "# Crea una lista de caracteres únicos en el texto.\n",
        "\n",
        "# Función 'set()' obteniee el conjunto de caracteres únicos del texto (eliminando duplicados).\n",
        "# Función 'sorted()' ordena esos caracteres en una lista.\n",
        "# Crea variable 'vocab' con todos los caracteres diferentes que aparecen en el texto\n",
        "vocab = sorted(set(text))\n",
        "\n",
        "# Imprime el número total de caracteres únicos en el texto, lo que da una idea de la diversidad de caracteres en el texto.\n",
        "print(f'{len(vocab)} caracteres únicos')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNnrKn_lL-IJ"
      },
      "source": [
        "## Procesamiento de texto"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFjSVAlWzf-N"
      },
      "source": [
        "### Vectorizar el texto\n",
        "\n",
        "Antes del entrenamiento, se deben convertir las secuencias de texto (strings) en una representación numérica.\n",
        "\n",
        "La capa `tf.keras.layers.StringLookup` convierte cada caracter en un ID numerico. Solo que primero se debe dividir el texto en tokens.\n",
        "\n",
        "\"Capa\" se refiere a un conjunto de neuronas que procesan datos en una red neuronal. En RN y DL, una capa es una estructura fundamental que toma una entrada, realiza alguna transformación o cálculo sobre ella y produce una salida.\n",
        "\n",
        "`tf.keras.layers.StringLookup` es una capa especializada en Keras que convierte cadenas en índices enteros basados en un vocabulario predefinido. No es una \"capa\" en el sentido tradicional de procesar características, sino más bien una herramienta para la preparación de datos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "a86OoYtO01go",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fac05d24-8cbd-4283-9251-7338fb665f81"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "# Se define una lista 'example_texts' con dos cadenas de caracteres: 'abcdefg' y 'xyz'.\n",
        "example_texts = ['abcdefg', 'xyz']\n",
        "\n",
        "# Utilizando la función 'unicode_split' de 'tf.strings', se descompone cada cadena en la lista 'example_texts' en sus caracteres individuales.\n",
        "# El argumento 'input_encoding' especifica la codificación de los caracteres en las cadenas; en este caso, se usa 'UTF-8'.\n",
        "chars = tf.strings.unicode_split(example_texts, input_encoding='UTF-8')\n",
        "\n",
        "# Se muestra el resultado de la operación anterior.\n",
        "chars\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1s4f1q3iqY8f"
      },
      "source": [
        "Ahora se crea la capa `tf.keras.layers.StringLookup`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "6GMlCe3qzaL9"
      },
      "outputs": [],
      "source": [
        "# Se crea una instancia de la capa 'StringLookup' de Keras, la cual convierte cadenas en índices enteros.\n",
        "# Esta capa es útil cuando se trabaja con texto y se desea convertir palabras o caracteres en números que puedan ser\n",
        "# procesados por un modelo de machine learning.\n",
        "#\n",
        "# Parámetros:\n",
        "#   - vocabulary: Especifica el vocabulario que se utilizará para la conversión. En este caso, se pasa la lista 'vocab'\n",
        "#     que contiene los caracteres únicos del texto.\n",
        "#   - mask_token: Al establecerlo en 'None', indica que no se desea usar un token de máscara. Un token de máscara\n",
        "#     generalmente se usa para representar valores desconocidos o faltantes, pero aquí se omite.\n",
        "\n",
        "ids_from_chars = tf.keras.layers.StringLookup(\n",
        "    vocabulary=list(vocab), mask_token=None)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZmX_jbgQqfOi"
      },
      "source": [
        "Se convierte de tokens a índices únicos de cada caracter:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "WLv5Q_2TC2pc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee8712f2-5390-464c-9329-467e12905af7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[40, 41, 42, 43, 44, 45, 46], [63, 64, 65]]>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "# Usando la capa 'ids_from_chars' que se creó previamente, se convierten los caracteres en 'chars' a sus respectivos índices enteros.\n",
        "# La capa 'ids_from_chars' fue definida para mapear cada carácter en el vocabulario a un índice entero único.\n",
        "# Al llamar a esta capa con la entrada 'chars' (que contiene las cadenas divididas en caracteres),\n",
        "# se obtiene un tensor de índices enteros que representan esos caracteres.\n",
        "ids = ids_from_chars(chars)\n",
        "\n",
        "# Se muestra el tensor resultante 'ids'. Este tensor contiene los índices enteros que corresponden a los caracteres en 'chars'.\n",
        "ids\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZfqhkYCymwX"
      },
      "source": [
        "Dado que el objetivo de este tutorial es generar texto, después de que el modelo produce una salida en forma de índices, será importante invertir esta representación a cadenas legibles por humanos.\n",
        "\n",
        "Para ello, puedes usar tf.keras.layers.StringLookup(..., invert=True).\n",
        "Al establecer el parámetro invert en True, le dices a la capa que quieres convertir índices a cadenas, en lugar de cadenas a índices."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uenivzwqsDhp"
      },
      "source": [
        "**Nota:**\n",
        "Aquí, en lugar de pasar el vocabulario original generado con `sorted(set(text))`, utiliza el método `get_vocabulary()` de la capa `tf.keras.layers.StringLookup` para que el token `[UNK]` se establezca de la misma manera.\n",
        "\n",
        "Cuando inviertes la representación, es crucial que uses el mismo vocabulario que se utilizó para la conversión original. La capa StringLookup puede añadir automáticamente un token especial llamado `[UNK]` para manejar caracteres o palabras desconocidas. El método `get_vocabulary()` proporcionará el vocabulario exacto que la capa está utilizando, incluyendo cualquier token especial como `[UNK]`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Wd2m3mqkDjRj"
      },
      "outputs": [],
      "source": [
        "# Crea una instancia de la capa 'StringLookup' de Keras, configurada para realizar la operación inversa: convertir índices enteros en cadenas.\n",
        "\n",
        "# Parámetros:\n",
        "#   - vocabulary: Especifica el vocabulario que utilizará la conversión, el método 'get_vocabulary()' de la capa 'ids_from_chars' garantiza que se tenga en cuenta cualquier token adicional como '[UNK]'\n",
        "#   - invert: 'True' indica que deseamos invertir la operación, i.e. convertir índices enteros en cadenas, y no al revés.\n",
        "#   - mask_token:'None' indica que no se desea usar un token de máscara para representar valores desconocidos o faltantes.\n",
        "chars_from_ids = tf.keras.layers.StringLookup(\n",
        "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqTDDxS-s-H8"
      },
      "source": [
        "Esta capa recupera caracteres del vector de índices, y los retorna como un `tf.RaggedTensor` de caracteres:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "c2GCh0ySD44s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c83ad0e6-ea8d-4d62-e182-6771409b10bc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "# variable cars se crea usando la capa recien creada para procesar tensor 'ids'\n",
        "chars = chars_from_ids(ids)\n",
        "chars"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-FeW5gqutT3o"
      },
      "source": [
        "Se puede usar `tf.strings.reduce_join` para juntar los carácteres en secuencias."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "zxYI-PeltqKP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31c93342-d2e3-43c7-ddcb-287d0fd2e20f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([b'abcdefg', b'xyz'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "# Función 'reduce_join' de 'tf.strings' concatena 'chars' a lo largo de la última dimensión (axis=-1)\n",
        "# Se convierte el resultado en un array de numpy con '.numpy()'.\n",
        "tf.strings.reduce_join(chars, axis=-1).numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "w5apvBDn9Ind"
      },
      "outputs": [],
      "source": [
        "# Define función 'text_from_ids' con objetivo de convertir un tensor de 'ids'  en texto, basado en el mapeo inverso de la capa 'chars_from_ids'.\n",
        "def text_from_ids(ids):\n",
        "    # Utiliza directamente la capa 'chars_from_ids' para convertir el tensor de índices 'ids' en tensor de caracteres.\n",
        "    # Con 'tf.strings.reduce_join' concatena caracteres a lo largo de la última dimensión para formar tensor de cadenas de texto.\n",
        "    return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbmsf23Bymwe"
      },
      "source": [
        "### La tarea de predicción"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wssHQ1oGymwe"
      },
      "source": [
        "Dado un caracter (letra) o una secuencia de caracteres, ¿cuál es el próximo caracter con mayor probabilidad? Esta es la tarea para la cual se está entrenando el modelo. La entrada al modelo será una secuencia de caracteres, y se lo entrenará para predecir una salida: el siguiente carácter en cada iteración.\n",
        "\n",
        "Dado que las RNNs mantienen un estado interno que depende de los elementos vistos previamente, considerando todos los caracteres procesados hasta el momento, ¿cuál es el siguiente caracter?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgsVvVxnymwf"
      },
      "source": [
        "### Crea ejemplos y objetivos para el entrenamiento\n",
        "\n",
        "Se divide el texto en secuencias ejemplo. Cada secuencia de entrada contendrá un número de caracteres dado por `seq_length`.\n",
        "\n",
        "Para cada \"input\", los \"targets\" contienen la misma longitud de texto, excepto que se desplaza un carácter a la derecha.\n",
        "\n",
        "Divide el texto en fragmentos con `seq_length+1` caracteres. Por ejemplo, supongamos que `seq_length` es 4 y nuestro texto es \"Hola\". La secuencia input sería \"Hola\", y la secuencia target sería \"ola \".\n",
        "\n",
        "Para hacer esto, primero utiliza la función `tf.data.Dataset.from_tensor_slices` para convertir el vector de texto en una secuencia de índices de caracteres."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "UopbsKi88tm5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2fa8c215-dabf-4170-8308-77ce15b56520"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1115394,), dtype=int64, numpy=array([19, 48, 57, ..., 46,  9,  1])>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "# 'unicode_split' de 'tf.strings' descompone 'text' en caracteres individuales usando Unicode 'UTF-8'.\n",
        "# 'ids_from_chars' convierte caracteres en índices enteros. Se almacena tensor resultante de índices enteros en 'all_ids'.\n",
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
        "\n",
        "# Tensor 'all_ids' contiene los índices enteros que representan cada carácter en el texto original.\n",
        "all_ids\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "qmxrYDCTy-eL"
      },
      "outputs": [],
      "source": [
        "# Crea objeto 'Dataset' de TensorFlow a partir del tensor 'all_ids'.\n",
        "# Función 'from_tensor_slices' toma tensor 'all_ids' y lo convierte en un 'Dataset', donde cada elemento es un índice entero del tensor original.\n",
        "# 'Dataset' facilita la manipulación, transformación y batching de los datos para el entrenamiento de modelos.\n",
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "cjH5v45-yqqH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7cd0e3d9-ff74-46eb-a264-f6ee31977475"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F\n",
            "i\n",
            "r\n",
            "s\n",
            "t\n",
            " \n",
            "C\n",
            "i\n",
            "t\n",
            "i\n",
            "z\n",
            "e\n",
            "n\n"
          ]
        }
      ],
      "source": [
        "# Itera sobre los primeros 13 elementos del objeto 'Dataset' 'ids_dataset'.\n",
        "# Cada índice lo convierte de vuelta a su carácter correspondiente usando la capa 'chars_from_ids'.\n",
        "# Decodifica el carácter resultante desde su representación binaria a una cadena UTF-8 y lo imprime.\n",
        "for ids in ids_dataset.take(13):\n",
        "    print(chars_from_ids(ids).numpy().decode('utf-8'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "C-G2oaTxy6km"
      },
      "outputs": [],
      "source": [
        "# establece variable 'seq_length' en 100. Cada secuencia ejemplo estará compuesta de 100 carácteres.\n",
        "seq_length = 100\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ZSYAcQV8OGP"
      },
      "source": [
        "El método `batch` permite convertir fácilmente estos carácteres individuales en secuencias del tamaño deseado."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "BpdjRO2CzOfZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07edfa82-9487-4c3c-b33c-91d0d6c15e04"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b'F' b'i' b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':'\n",
            " b'\\n' b'B' b'e' b'f' b'o' b'r' b'e' b' ' b'w' b'e' b' ' b'p' b'r' b'o'\n",
            " b'c' b'e' b'e' b'd' b' ' b'a' b'n' b'y' b' ' b'f' b'u' b'r' b't' b'h'\n",
            " b'e' b'r' b',' b' ' b'h' b'e' b'a' b'r' b' ' b'm' b'e' b' ' b's' b'p'\n",
            " b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'A' b'l' b'l' b':' b'\\n' b'S' b'p' b'e'\n",
            " b'a' b'k' b',' b' ' b's' b'p' b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'F' b'i'\n",
            " b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':' b'\\n' b'Y'\n",
            " b'o' b'u' b' '], shape=(101,), dtype=string)\n"
          ]
        }
      ],
      "source": [
        "# Variable 'sequences' contendrá subconjuntos (\"batches\") de 'ids_dataset', de 101 elemento de largo, utilizando la función '.batch()'.\n",
        "# 'seq_length+1' establece el tamaño de los parches (100+1) y 'drop_remainder=True' asegura descarta cualquier remanente que no permita cumplir con ese tamaño (todos los subconjuntos quedan de 101 elementos).\n",
        "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "#Itera sobre la primera secuencia de 101 elementos y la imprime como caracteres\n",
        "for seq in sequences.take(1):\n",
        "  print(chars_from_ids(seq))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5PHW902-4oZt"
      },
      "source": [
        "\n",
        "Es más fácil ver lo que se está haciendo si se unen los tokens de nuevo en secuencias:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "QO32cMWu4a06",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91841967-e694-492d-9dba-b74d2732526b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
            "b'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
            "b\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
            "b\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
            "b'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
          ]
        }
      ],
      "source": [
        "# Itera sobre las primeras 5 secuencias y las imprime como cadenas de texto utilizando la función 'text_from_ids' definida en [34]\n",
        "for seq in sequences.take(5):\n",
        "  print(text_from_ids(seq).numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UbLcIPBj_mWZ"
      },
      "source": [
        "Para entrenar el modelo se necesitará un conjunto de datos formado por pares `(input, label)`. donde `input` y `label` son secuencias. En cada iteración, el input es el caracter actual y label es el siguiente caracter.\n",
        "\n",
        "La siguiente función toma una secuencia como argumento, la duplica, y la desplaza para alinear el input y el label para cada iteración:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "9NGu-FkO_kYU"
      },
      "outputs": [],
      "source": [
        "# se define la función 'split_input_target' que toma una secuencia ('sequence') como argumento y retorna 'input_text' y 'target_text'\n",
        "# 'input_text' es la sequencia menos el último caracter\n",
        "# 'target_text' es la sequencia a partir del segundo caracter\n",
        "def split_input_target(sequence):\n",
        "    input_text = sequence[:-1]\n",
        "    target_text = sequence[1:]\n",
        "    return input_text, target_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "WxbDTJTw5u_P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc847fd3-0d52-49f4-b1ed-7fb921e0bfe4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['T', 'e', 'n', 's', 'o', 'r', 'f', 'l', 'o'],\n",
              " ['e', 'n', 's', 'o', 'r', 'f', 'l', 'o', 'w'])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "# Se aplica funcion a \"TensorFlow\" como ejemplo:\n",
        "split_input_target(list(\"Tensorflow\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "B9iKPXkw5xwa"
      },
      "outputs": [],
      "source": [
        "# Utiliza método 'map' para aplicar la función 'split_input_target' a cada elemento del objeto 'Dataset' 'sequences'\n",
        "# 'dataset' es un nuevo objeto 'Dataset' donde cada elemento es una tupla compuesta por una secuencia de entrada (input) y una secuencia objetivo (target).\n",
        "dataset = sequences.map(split_input_target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "GNbw-iR0ymwj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54440af4-f559-4b39-adad-e305cb99b14b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input : b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
            "Target: b'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
          ]
        }
      ],
      "source": [
        "# Itera la primera tupla en 'dataset' para mostrar las secuencias 'input_example' y 'target_example'\n",
        "for input_example, target_example in dataset.take(1):\n",
        "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
        "    print(\"Target:\", text_from_ids(target_example).numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJdfPmdqzf-R"
      },
      "source": [
        "### Create training batches\n",
        "\n",
        "Se uso `tf.data` para dividir el texto en secuencias manejables. Pero antes de alimentar estos datos al modelo, es necesario reordenar aleatoriamente los datos -crucial para asegurar que el modelo no aprenda patrones indeseados basado en el orden de los datos-, y agruparlos en lotes -subconjuntos de datos que se alimenta al modelo en una sola iteración para que el entrenamiento sea más eficiente en GPU's-"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "p2pGotuNzf-S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4305fed-239f-43a3-d1af-191bc595ef1f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "# Constante 'BATCH_SIZE' indica el número de secuencias que se incluirán en cada lote durante el entrenamiento (64).\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# 'BUFFER_SIZE' determina el número de elementos que se deben cargar en memoria para ser mezclados.\n",
        "# Librería tf.data está diseñada para trabajar con secuencias que pueden ser infinitamente largas, por lo que no intenta mezclar toda la secuencia en memoria.\n",
        "# En su lugar, mantiene un buffer (en este caso de tamaño 10,000) y mezcla elementos dentro de ese buffer.\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "# Se realizan varias transformaciones en el 'Dataset' 'dataset':\n",
        "dataset = (\n",
        "    dataset\n",
        "    # 'shuffle' mezcla aleatoriamente los elementos del 'Dataset' usando un buffer de tamaño 'BUFFER_SIZE'.\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    # 'batch' agrupa los datos en lotes de tamaño 'BATCH_SIZE'.\n",
        "    # 'drop_remainder=True' asegura que si un lote no alcanza tamaño 64 (p. e., al final del 'Dataset') se descarte.\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    # 'prefetch' permite que el modelo de entrenamiento y el proceso de carga de datos se solapen, mejorando la eficiencia del entrenamiento.\n",
        "    # 'tf.data.experimental.AUTOTUNE' permite a TensorFlow decidir automáticamente cuántos lotes deben ser pre-cargados.\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
        "\n",
        "# Se muestra el objeto 'Dataset' transformado 'dataset'.\n",
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6oUuElIMgVx"
      },
      "source": [
        "## Construcción del modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8gPwEjRzf-Z"
      },
      "source": [
        "En esta sección se define el modelo como una subclase de `keras.Model` (Para más detalles vea [Making new Layers and Models via subclassing](https://www.tensorflow.org/guide/keras/custom_layers_and_models)).\n",
        "\n",
        "Este modelo tiene tres capas:\n",
        "\n",
        "* `tf.keras.layers.Embedding`:  La capa de entrada. Una tabla de búsqueda entrenable que mapeará cada ID de carácter a un vector con dimensiones  `embedding_dim`\n",
        "* `tf.keras.layers.GRU`: Un tipo de RNN de tamaño `units=rnn_units` (También puedes usarse una capa LSTM aquí).\n",
        "* `tf.keras.layers.Dense`:  La capa de salida, con `vocab_size` salidas. Produce un logit para cada carácter en el vocabulario. Estos son la log-verosimilitud de cada carácter según el modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "zHT8cLh7EAsg"
      },
      "outputs": [],
      "source": [
        "# Se establece la longitud del vocabulario obtenida de 'ids_from_chars'\n",
        "vocab_size = len(ids_from_chars.get_vocabulary())\n",
        "\n",
        "# Se establece la dimensión de los vectores de entrada\n",
        "embedding_dim = 256\n",
        "\n",
        "# Se establece número de unidades neuronales en la GRU (Gated Recurrent Unit)\n",
        "rnn_units = 1024"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "wj8HQ2w8z4iO"
      },
      "outputs": [],
      "source": [
        "# Se define una clase 'MyModel' que hereda de 'tf.keras.Model', que significa que este modelo personalizado se beneficiará de características y funcionalidades del modelo base de Keras.\n",
        "class MyModel(tf.keras.Model):\n",
        "\n",
        "  # Método constructor de la clase.\n",
        "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "    # Se llama al constructor de la clase base 'tf.keras.Model'.\n",
        "    super().__init__(self)\n",
        "    # Se define la capa 'embedding' que transformará los índices de caracteres en vectores densos.\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    # Se define la capa 'gru', que es una red neuronal recurrente (GRU).\n",
        "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
        "                                   return_sequences=True,  # La capa devolverá la secuencia completa y no solo el último output.\n",
        "                                   return_state=True)      # La capa devolverá el estado final además del output.\n",
        "    # Se define la capa 'dense', que producirá logits para cada carácter en el vocabulario.\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  # Método 'call' define la lógica de propagación hacia adelante del modelo.\n",
        "  def call(self, inputs, states=None, return_state=False, training=False):\n",
        "    x = inputs\n",
        "    # Se pasa la entrada a través de la capa 'embedding'.\n",
        "    x = self.embedding(x, training=training)\n",
        "    # Se verifica si se proporciona un estado inicial. Si no, se obtiene el estado inicial de la capa 'gru'.\n",
        "    if states is None:\n",
        "      states = self.gru.get_initial_state(x)\n",
        "    # Se pasa la salida de la capa 'embedding' a través de la capa 'gru'.\n",
        "    x, states = self.gru(x, initial_state=states, training=training)\n",
        "    # Se pasa la salida de la capa 'gru' a través de la capa 'dense'.\n",
        "    x = self.dense(x, training=training)\n",
        "\n",
        "  # Si 'return_state' es True, se devuelve tanto la salida como el estado. Si es False, solo se devuelve la salida.\n",
        "    if return_state:\n",
        "      return x, states\n",
        "    else:\n",
        "      return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "IX58Xj9z47Aw"
      },
      "outputs": [],
      "source": [
        "# Se crea una instancia del modelo 'MyModel', proporcionando los parámetros necesarios para su construcción.\n",
        "model = MyModel(\n",
        "    vocab_size=vocab_size,\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RkA5upJIJ7W7"
      },
      "source": [
        "Para cada caracter, el modelo busca el embedding, ejecuta una iteración del GRU con el embedding como entrada, y aplica la capa densa para generar logits que predicen la log-verosimilitud del siguiente caracter:\n",
        "\n",
        "![A drawing of the data passing through the model](https://github.com/tensorflow/text/blob/master/docs/tutorials/images/text_generation_training.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gKbfm04amhXk"
      },
      "source": [
        "**Note:** Para el entrenamiento puede usarse un modelo `keras.Sequential`. Sin embargo, para generar texto, se necesitará gestionar el estado interno de la RNN. Es más sencillo incluir las opciones de entrada y salida del estado desde el principio, que reorganizar la arquitectura del modelo más tarde. Para más detalles, consulta la [Guía de RNN de Keras](https://www.tensorflow.org/guide/keras/rnn#rnn_state_reuse)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ubPo0_9Prjb"
      },
      "source": [
        "## Prueba del modelo\n",
        "\n",
        "Ahora se corre el modelo para asegurarse de que se comporta de la manera esperada.\n",
        "\n",
        "Se revisa primero las dimensiones del output:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "C-_70kKAPrPU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a024b07-2e0c-49f1-f073-aaab8101ee65"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 100, 66) # (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ],
      "source": [
        "# Se itera sobre un único lote (batch) del conjunto de datos 'dataset'.\n",
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "\n",
        "    # Se utiliza el modelo 'model' para obtener las predicciones del lote de entrada 'input_example_batch'.\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "\n",
        "    # Se imprime la forma (shape) de las predicciones obtenidas y una explicación de cada dimensión.\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q6NzLBi4VM4o"
      },
      "source": [
        "En este ejemplo, la longitud de la secuencia de entrada es`100`, pero el modelo puede ser ejecutado con entradas de cualquier longitud:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "vPGmAAXmVLGC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6cad7647-cd6a-4b54-8971-8d472cbed664"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"my_model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       multiple                  16896     \n",
            "                                                                 \n",
            " gru (GRU)                   multiple                  3938304   \n",
            "                                                                 \n",
            " dense (Dense)               multiple                  67650     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,022,850\n",
            "Trainable params: 4,022,850\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Resumen de especificaciones del modelo\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwv0gEkURfx1"
      },
      "source": [
        "Para obtener predicciones reales del modelo, se necesita tomar muestras de la distribución de salida, para obtener índices de caracteres reales. Esta distribución está definida por los logits sobre el vocabulario de caracteres.\n",
        "\n",
        "Nota: Es importante tomar muestras de esta distribución ya que tomar el argmax de la distribución puede hacer que el modelo quede atrapado fácilmente en un bucle\n",
        "\n",
        "Se prueba para el primer ejemplo en el lote:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "4V4MfFg0RQJg"
      },
      "outputs": [],
      "source": [
        "# Se utiliza la función 'tf.random.categorical' para muestrear índices a partir de las predicciones del modelo.\n",
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "\n",
        "# Se elimina una dimensión innecesaria de 'sampled_indices' y se convierte el tensor resultante a un array de NumPy.\n",
        "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QM1Vbxs_URw5"
      },
      "source": [
        "Esto nos da, en cada iteración, una predicción del siguiente caracter en el índice:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "YqFMUQc_UFgM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8bd7fa97-784e-40a7-b90f-c79a58bc145a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 1, 11,  2, 30,  7, 64, 12, 59,  2, 33, 18,  8, 52, 20, 13,  8,  3,\n",
              "       60, 47, 25, 47, 30, 28, 31, 57, 64, 26, 52, 28, 20, 21, 65, 55, 59,\n",
              "       52, 30, 45, 40, 13,  5, 25, 14, 54, 49, 24, 27, 42, 40, 15, 31, 19,\n",
              "       19,  9, 31,  4, 30,  0, 30, 24, 16,  3, 55, 20, 42, 43, 21, 45, 58,\n",
              "       28, 27, 55, 25, 35, 62,  0, 28,  8, 38, 34, 58, 41,  1, 35, 38,  0,\n",
              "        1, 33, 28, 10, 63, 11, 58,  8, 55, 64,  7, 49, 38, 57, 11])"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "sampled_indices"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LfLtsP3mUhCG"
      },
      "source": [
        "Se decodifica para observar el texto predicho por este modelo sin entrenamiento:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "xWcFwPwLSo05",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a8b86b5-456e-4a61-84bf-d031b778e8fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:\n",
            " b'll will come to nought,\\nWhen such bad dealings must be seen in thought.\\n\\nGLOUCESTER:\\nHow now, my lor'\n",
            "\n",
            "Predicción del próximo caracter:\n",
            " b'\\n: Q,y;t TE-mG?-!uhLhQORryMmOGHzptmQfa?&LAojKNcaBRFF.R$Q[UNK]QKC!pGcdHfsONpLVw[UNK]O-YUsb\\nVY[UNK]\\nTO3x:s-py,jYr:'\n"
          ]
        }
      ],
      "source": [
        "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\n",
        "print()\n",
        "print(\"Predicción del próximo caracter:\\n\", text_from_ids(sampled_indices).numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJL0Q0YPY6Ee"
      },
      "source": [
        "## Entrenamiento del modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YCbHQHiaa4Ic"
      },
      "source": [
        "En este punto, el problema puede ser tratado como un problema de clasificación estandar. Dado el estado previo de la RNN, y la entrada en dicha iteración, se predice la clase del siguiente caracter."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trpqTWyvk0nr"
      },
      "source": [
        "### Adjunta un optimizador y una función de perdida\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UAjbjY03eiQ4"
      },
      "source": [
        "La \"pérdida\" es una medida escalar que indica cuán bien (o mal) está funcionando un modelo en un conjunto de datos específico. Se calcula comparando las predicciones del modelo con las verdaderas etiquetas o valores objetivo. Durante el entrenamiento de un modelo, el objetivo principal es minimizar esta pérdida, ajustando los parámetros del modelo (como los pesos y sesgos en una red neuronal).\n",
        "\n",
        "La función de perdida estandar `tf.keras.losses.sparse_categorical_crossentropy` funciona en este caso porque es se aplica a través de la última dimensión de las predicciones.\n",
        "\n",
        "Dado que tu modelo devuelve logits, necesitas establecer el indicador  `from_logits`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "ZOeWdgxNFDXq"
      },
      "outputs": [],
      "source": [
        "# Define la función de pérdida 'SparseCategoricalCrossentropy', especificando que las predicciones proporcionadas serán logits.\n",
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "4HrXTACTdzY-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1abb2f75-8cfd-4ef2-c5f7-80dbcf74ba46"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dimensiones de la predicción:  (64, 100, 66)  # (batch_size, sequence_length, vocab_size)\n",
            "Perdida promedio:         tf.Tensor(4.1904016, shape=(), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "# Se calcula la pérdida promedio para un lote de ejemplos, utilizando las predicciones del modelo y las verdaderas etiquetas objetivo.\n",
        "example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)\n",
        "\n",
        "# Se imprime la forma de las predicciones y la pérdida promedio calculada para el lote.\n",
        "print(\"Dimensiones de la predicción: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"Perdida promedio:        \", example_batch_mean_loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkvUIneTFiow"
      },
      "source": [
        "Un modelo inicializado recientemente no debería estar muy seguro de si mismo, por lo que los logits resultado deberían todos tener magnitudes ssimilares.\n",
        "\n",
        "Para confirmar esto, se puede revisar que el exponente de la perdida promedio es aproximadamente igual al tamaño del vocabulario. Una perdida mucho mayor significa que el modelo está seguro de sus respuestas erroneas, y está mal inicializado:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "MAJfS5YoFiHf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba92c209-0bf6-4a3e-cc80-014f42b5c98a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "66.04931"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ],
      "source": [
        "# se eleva a la e la perdida promedio del conjunto ejemplo, que da aproximadamente igual al tamaño del vocabulario (66)\n",
        "tf.exp(example_batch_mean_loss).numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jeOXriLcymww"
      },
      "source": [
        "Se configura el procedimiento de entrenamiento con el método `tf.keras.Model.compile`. Se usa `tf.keras.optimizers.Adam` con argumentos default y la funnción de perdida."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "DDl1_Een6rL0"
      },
      "outputs": [],
      "source": [
        "# Se compila el modelo especificando el optimizador y la función de pérdida.\n",
        "model.compile(optimizer='adam', loss=loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ieSJdchZggUj"
      },
      "source": [
        "### Configurar puntos de control"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C6XBUUavgF56"
      },
      "source": [
        "Se usa un `tf.keras.callbacks.ModelCheckpoint` para asegurar que se guardan puntos de control durante el entrenamiento:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "W6fWTriUZP-n"
      },
      "outputs": [],
      "source": [
        "# Se establece el directorio donde se guardaran los puntos de control\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Se configura el nombre de los puntos de control  (época -epoch- se refiere a un ciclo completo a través del conjunto de datos de entrenamiento.)\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "# Se configura el callback para guardar los pesos del modelo después de cada época.\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Ky3F_BhgkTW"
      },
      "source": [
        "### Ejecución del entrenamiento"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IxdOA-rgyGvs"
      },
      "source": [
        "Para mantener el tiempo de entrenamiento razonable, se usan 20 épocas para entrenar el modelo (el modelo ha tenido la oportunidad de aprender de cada muestra en el conjunto de entrenamiento 20 veces)\n",
        "\n",
        "En Colab, configura el entorno de ejecución a GPU para un entrenamiento más rápido."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "7yGBE2zxMMHs"
      },
      "outputs": [],
      "source": [
        "# Constante 'EPOCHS' se configura en 20\n",
        "EPOCHS = 20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "UK-hmKjYVoll",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8163611f-1801-4303-e8f9-36539c99035e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "172/172 [==============================] - 15s 58ms/step - loss: 2.7573\n",
            "Epoch 2/20\n",
            "172/172 [==============================] - 11s 51ms/step - loss: 2.0146\n",
            "Epoch 3/20\n",
            "172/172 [==============================] - 12s 52ms/step - loss: 1.7315\n",
            "Epoch 4/20\n",
            "172/172 [==============================] - 11s 53ms/step - loss: 1.5675\n",
            "Epoch 5/20\n",
            "172/172 [==============================] - 12s 56ms/step - loss: 1.4659\n",
            "Epoch 6/20\n",
            "172/172 [==============================] - 12s 55ms/step - loss: 1.3965\n",
            "Epoch 7/20\n",
            "172/172 [==============================] - 13s 55ms/step - loss: 1.3428\n",
            "Epoch 8/20\n",
            "172/172 [==============================] - 13s 58ms/step - loss: 1.2976\n",
            "Epoch 9/20\n",
            "172/172 [==============================] - 12s 56ms/step - loss: 1.2565\n",
            "Epoch 10/20\n",
            "172/172 [==============================] - 12s 58ms/step - loss: 1.2181\n",
            "Epoch 11/20\n",
            "172/172 [==============================] - 13s 58ms/step - loss: 1.1790\n",
            "Epoch 12/20\n",
            "172/172 [==============================] - 13s 60ms/step - loss: 1.1395\n",
            "Epoch 13/20\n",
            "172/172 [==============================] - 12s 58ms/step - loss: 1.0970\n",
            "Epoch 14/20\n",
            "172/172 [==============================] - 14s 57ms/step - loss: 1.0536\n",
            "Epoch 15/20\n",
            "172/172 [==============================] - 13s 58ms/step - loss: 1.0071\n",
            "Epoch 16/20\n",
            "172/172 [==============================] - 12s 59ms/step - loss: 0.9571\n",
            "Epoch 17/20\n",
            "172/172 [==============================] - 13s 60ms/step - loss: 0.9049\n",
            "Epoch 18/20\n",
            "172/172 [==============================] - 12s 60ms/step - loss: 0.8528\n",
            "Epoch 19/20\n",
            "172/172 [==============================] - 12s 57ms/step - loss: 0.7997\n",
            "Epoch 20/20\n",
            "172/172 [==============================] - 12s 58ms/step - loss: 0.7502\n"
          ]
        }
      ],
      "source": [
        "# Se inicia el proceso de entrenamiento del modelo utilizando el conjunto de datos y se configura el número de épocas y los callbacks.\n",
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "En resumen, el anterior fragmento de código inicia el entrenamiento del modelo utilizando el conjunto de datos `dataset` y el número de épocas especificado en `EPOCHS`. También configura el entrenamiento para que guarde los checkpoints del modelo después de cada época utilizando el callback `checkpoint_callback`. La información sobre el proceso de entrenamiento se guarda en la variable `history`."
      ],
      "metadata": {
        "id": "3FjA8K_X9dar"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKkD5M6eoSiN"
      },
      "source": [
        "## Generación de texto"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oIdQ8c8NvMzV"
      },
      "source": [
        "La manera más sencilla de generar texto con este modelo es correrlo en bucle, y mantener seguimiento al estado interno del modelo mientras se ejecuta.\n",
        "\n",
        "![Para generar texto, la salida del modelo se introduce de nuevo como entrada](https://github.com/tensorflow/text/blob/master/docs/tutorials/images/text_generation_sampling.png?raw=1)\n",
        "\n",
        "Cada vez que se llama el modelo se le pasa un texto y un estado interno. El modelo retorna una predicción del siguiente caracter y su nuevo estado. Se pasa la predicción y el estado de nuevo al modelo para seguir generando texto."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DjGz1tDkzf-u"
      },
      "source": [
        "El siguiente código hace una predicción de una sola iteración:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "iSBU1tHmlUSs"
      },
      "outputs": [],
      "source": [
        "# Se define una clase 'OneStep' que extiende 'tf.keras.Model' para la generación de texto.\n",
        "class OneStep(tf.keras.Model):\n",
        "\n",
        "  # Método de inicialización de la clase.\n",
        "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "    super().__init__()  # Llama al método de inicialización de la superclase.\n",
        "    self.temperature = temperature  # Configura la temperatura para modificar la probabilidad de las predicciones, seteada en 1.0.\n",
        "    self.model = model  # Modelo previamente entrenado.\n",
        "    self.chars_from_ids = chars_from_ids  # Función para obtener caracteres a partir de IDs.\n",
        "    self.ids_from_chars = ids_from_chars  # Función para obtener IDs a partir de caracteres.\n",
        "\n",
        "    # Crea una máscara para evitar que se genere \"[UNK]\" (desconocido).\n",
        "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "        values=[-float('inf')]*len(skip_ids),  # Pone un -infinito en cada índice no deseado.\n",
        "        indices=skip_ids,\n",
        "        dense_shape=[len(ids_from_chars.get_vocabulary())])  # Hace coincidir la forma con el vocabulario.\n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)  # Convierte el tensor disperso en denso.\n",
        "\n",
        "  # Método para generar un paso (carácter) basado en una entrada dada.\n",
        "  @tf.function\n",
        "  def generate_one_step(self, inputs, states=None):\n",
        "    # Convierte las cadenas en IDs de token.\n",
        "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "    # Ejecuta el modelo. Obtiene los logits predichos y los estados.\n",
        "    predicted_logits, states = self.model(inputs=input_ids, states=states, return_state=True)\n",
        "    predicted_logits = predicted_logits[:, -1, :]  # Solo usa la última predicción.\n",
        "    predicted_logits = predicted_logits/self.temperature  # Modifica los logits según la temperatura.\n",
        "\n",
        "    # Aplica la máscara de predicción para evitar generar \"[UNK]\".\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "    # Muestrea los logits de salida para generar IDs de token.\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)  # Elimina la dimensión innecesaria.\n",
        "\n",
        "    # Convierte de IDs de token a caracteres.\n",
        "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "\n",
        "    # Devuelve los caracteres predichos y el estado del modelo.\n",
        "    return predicted_chars, states\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "fqMOuDutnOxK"
      },
      "outputs": [],
      "source": [
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9yDoa0G3IgQ"
      },
      "source": [
        "Se ejecuta en bucle para generar texto. Al observar el texto generado, se ve que el modelo sabe cuand poner una mayúscula, hacer párrafos e imitar vocabulario como el que usa Shakespeare. Con un pequeño número de épocas de entrenamiento, aún no aprende a cómo formar oraciones coherentes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "ST7PSyk9t1mT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a010291-afbb-40bb-bde5-54162b101486"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROMEO:\n",
            "Cord herrow from an old tale both with thee?\n",
            "\n",
            "JULIET:\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "Thou liest! come, dwell up your met,\n",
            "Unfinister'd to a shrewdlike of\n",
            "them. Camillo term these friends? Your Mowbray fight!\n",
            "O, she is bold of sorrow and to Causes Warwick's daughter\n",
            "Cloudy colours in his father's legs that very virtue.\n",
            "I thought thou hadst breathed steel your signories:\n",
            "Baiders war with Mowbray restend, I tender\n",
            "And spirit to any of my troth, my\n",
            "Away before you was ne'er to bower. Pray, girl:\n",
            "Contempt at thy wearth! blest see an envy\n",
            "Was, senators will request your grace.\n",
            "\n",
            "GLOUCESTER:\n",
            "My lord, your brother are my womanish fear:\n",
            "Go, counted so the wars.\n",
            "\n",
            "Ligut Watchman:\n",
            "Here, sir! My blood should could be of forward;\n",
            "On Titus, O fair issue!\n",
            "\n",
            "LEONTES:\n",
            "Once more!' the virty nor hold?\n",
            "\n",
            "CLAUDIO:\n",
            "Ay, sir; sound, sir, it is a wolves to be a male to love,\n",
            "I come in bring their coward.\n",
            "\n",
            "MERCUTIO:\n",
            "A laughter; yet hath promised me worthy thief,\n",
            "See, wast the tale as thy sacred spring-of blins,\n",
            "An envious from  \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 3.4505791664123535\n"
          ]
        }
      ],
      "source": [
        "# Se registra el tiempo inicial para medir cuánto tiempo toma el proceso de generación.\n",
        "start = time.time()\n",
        "\n",
        "# Se inicializa el estado del modelo a 'None'.\n",
        "states = None\n",
        "# Se establece el carácter inicial como 'ROMEO:'.\n",
        "next_char = tf.constant(['ROMEO:'])\n",
        "# Se inicializa una lista con el carácter inicial para almacenar el resultado generado.\n",
        "result = [next_char]\n",
        "\n",
        "# Se generan 1000 caracteres usando el modelo 'one_step_model'.\n",
        "for n in range(1000):\n",
        "  # Se genera el siguiente carácter y se actualiza el estado del modelo.\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  # Se añade el carácter generado a la lista de resultados.\n",
        "  result.append(next_char)\n",
        "\n",
        "# Se unen todos los caracteres generados en una sola cadena.\n",
        "result = tf.strings.join(result)\n",
        "\n",
        "# Se registra el tiempo final.\n",
        "end = time.time()\n",
        "\n",
        "# Se imprime el texto generado y el tiempo que tomó el proceso.\n",
        "#'result[0].numpy().decode('utf-8') convierte el tensor de TensorFlow a una cadena de Python.\n",
        "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AM2Uma_-yVIq"
      },
      "source": [
        "La manera más sencilla para mejorar los resultados es entrenar el modelo por más epocas (intente `EPOCHS = 30`).\n",
        "\n",
        "También puede experimentarse comenzando con una entrada diferente, añadiendo otra capa de RNN para mejorar la precisión del modelo, o ajustar la termperatura para generar predicciones más o menos aleatorias."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_OfbI4aULmuj"
      },
      "source": [
        "Si se quiere que el modelo produzca testo *más rápido*, la manera más sencilla es lotear (\"batch\") la generación de texto. En el ejemplo de abajo, el modelo genera 5 resultados en el mismo tiempo que toma generar 1 en las líneas anteriores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "ZkLu7Y8UCMT7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "555ff607-ab56-4462-b8c0-396c4786c80a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b\"ROMEO:\\nGood ladies, you being there beastly, speak 'tis great so hate\\nShall commend me to your jaultion,\\nIn pardonious very prey. Now arm yourself.\\n\\nMISTRESS OVERDONE:\\nWhats, marry, master, marry, as he had said,\\nWith tongue is bounded to his sacred blood,\\nWithout awaken upon our fair equally.\\nThink up your subjects?\\n\\nSecond Murderer:\\nI did; but all this waiting,\\nWhere bloody will we marry her; and the\\nserving-mere, is an envious eye.\\n\\nQUEEN ELIZABETH:\\nWilt thou buside behind of rose,\\nSo with my hate to save my mind, eat no\\nnot Kate and weep not, not deceived.\\n\\nKING RICHARD II:\\nA periorafe-bond I spake, and not woo'-take.\\n\\nVIRGILIA:\\nIndeed, mine old fast for a lie:\\nYou're for the entertain thus.\\n\\nDUKE VINCENTIO:\\nThere is our general.\\n\\nMARCIUS:\\nI go, my father, I have been well to revel me in hatrate\\nAnd we must purchase till thee.\\nI must be gone and not advance these power\\nTo do your good time out of the sacrament,\\nAnd privy to the penitent thereligh.\\n\\nCLARENCE:\\nReason, I would take warry al\"\n",
            " b\"ROMEO:\\nYet Edward, let us all this finger of some lews\\nDeposed. Therefore you shall\\na young word richess in my wrongs: were they should be,\\nSink, my save your last embrower, sir.\\n\\nESCALUS:\\nWould I were then could never be so mark a tower;\\nAnd bowh he is to a stand were tasted:\\nThe Kate has welcome where the true deal of reas,\\nBear himself falsely taken; we were ask one\\nWho, up sweet Madagou' thee, By Dorthrew;\\nFor by us all, and most good help,\\nBecause she bid live death, and therefore were not\\nIs, in good time.\\n\\nFirst Citizen:\\nWe cannot be much behind that this is very place.\\nWill you so old of this, bestrowing a part\\nThings that rests in my timeless ring of such\\nanother, then, or quick, thou wert down to see\\nThe sharps are rushing in him which in hell.\\n\\nHASTINGS:\\nHow doth my soul? Tell him, nor ear to London persua\\nCast off it is: betit my honour with myself\\nTo be your consumer. What is it?\\n\\nBAGOT:\\nWe speak should be deliver'd.\\n\\nDUKE VINCENTIO:\\nYou will a traitor to the crown.\\n\\nDUKE VINCEN\"\n",
            " b\"ROMEO:\\nAnd, towards Chertsey, whose gratitude took horses?\\n\\nCATESBY:\\nMadam, we shroud us in him!\\n\\nBUCKINGHAM:\\nThen have my father's height there was no: now.\\n\\nCORIOLANUS:\\nAs I but hear you, father; yea, my good lord--\\n\\nCAMILLO:\\nShould I be resolute a spirit!\\nWho ired us and threaten precious.\\n\\nBRUTUS:\\nCall them away work you thus hath not it.\\nNow, for young Madam Ground,--Nurse with a fairest dear\\nWater-look'd for such a swooned\\nKing Henry's friend of Cauling from the King of Signior Vartine?\\n\\nKING EDWARD IV:\\nBut if you mar, noy almost fresh presence often,\\nFor they the strong surmorts as Plours and are as bowns\\nTo horse by pounds to the palace; he\\nhas left you for your punishmen: tells her son.\\n\\nJUCIET:\\nMy honest good apter once contrive them as the death.\\n\\nJULIET:\\nAy, my good lord; I chief thou writ properlant\\nThat whose journeys we have scours:\\nYet ever were brigfflet beaution in she,\\nBe eat not her to-night. Peopab,\\nTo bear a woman's tent your foe and arms\\nTo make a carpuner of inferior,\"\n",
            " b\"ROMEO:\\nBecause thy uncles fell it were a two\\nKate, Angelo. Romeo, making me, and with\\nThe yards that kill-fineful and a tackle's pentle\\nRelension; all free faults ourselves:\\nOur points o' the east, and tell them who\\nThe messen and a day in war,\\nThus with a dark of us, would I were some cause\\nWhere stands the care to Cobulita.\\nWithin their climates, or mildly, and a foil\\nword ill coss, behind, reign and water-with\\nThat rebels me both worthy fellow.\\n\\nThird Conseiver:\\nMy exary you.\\n\\nFirst Murderer:\\nI say; conduct you stay my bones:\\nSome on the earth hath held out Roge, alack, for\\nHurtly from the townraw of the tale;\\nOne kiss your hand; therefore, I pine all this.\\n\\nHASTINGS:\\nOn me, whose loss of this can make, thou like me fain?\\nO, when we shall be so rid his legs.\\n\\nRICHARD:\\nAs I by thing, I should come again.\\nThen call him like a barbind time and them\\nTo his simple ignory\\nThat many a tale proceed. Let me prove saints\\nFor charity, and I must wait upon your ear\\nAnd neit his guess. Therefore is yo\"\n",
            " b\"ROMEO:\\nGrow to the ense, exercise, will you negls--\\nWhat shouts it beais a urmindred; but chair no more\\nmust be thy sight; and is he match in eaten blows,\\nShalt be a cured with tears are as mere were\\nAgainst my word he lost. A sister shall be thine.\\n\\nQUEEN ELIZABETH:\\nNo, no: the satral fathers' brothers broaded\\nWith one too curd on angel on the child.\\n\\nRICHARD:\\nServants, lead answer; though it goed from them,\\nTo say your first dief, eyes' crowns,\\nWhen he that still stumbler that the priest\\nOf their very death, which is mortal town;\\nAnd, having now Frolinal and wars.\\n\\nROMEO:\\nAnd that has whistle, till you slander him.\\n\\nJULIET:\\nMy queen and souls for Clarence, worstio and drunkench\\nUnhorrow diseased with badod:' fierce of him\\nAnd see how now! which thou art too far.\\n\\nPETRUCHIO:\\nTrail your ill-coss'd, and give me leave them beginn,\\nThan feed is this by lard, though it make power\\nWhich leisure from heaveng's upon thy blood;\\nWhere, where thou diest, then so shrive I have begun.\\n\\nSERBSSTAS:\\nI have\"], shape=(5,), dtype=string) \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 3.1556389331817627\n"
          ]
        }
      ],
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "\n",
        "# Se establecen cinco cadenas iniciales, todas con el valor 'ROMEO:'.\n",
        "next_char = tf.constant(['ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "# Se generan 1000 caracteres para cada una de las cadenas iniciales usando el modelo 'one_step_model'.\n",
        "for n in range(1000):\n",
        "  # Se generan los siguientes caracteres y se actualiza el estado del modelo.\n",
        "  # Dado que hay cinco cadenas iniciales, en cada iteración se generan cinco caracteres.\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "\n",
        "  # Se añaden los caracteres generados a la lista de resultados.\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result, '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UlUQzwu6EXam"
      },
      "source": [
        "## Exportar el generador\n",
        "\n",
        "Este modelo de un solo paso puede ser [ guardado y restaurado](https://www.tensorflow.org/guide/saved_model) fácilmente, lo que permite usarlo en cualquier lugar donde se acepte un `tf.saved_model`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "3Grk32H_CzsC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bbbe4ac2-000c-4ec8-ce1d-0f5c5a6ad918"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Skipping full serialization of Keras layer <__main__.OneStep object at 0x7e95f0cdc730>, because it is not built.\n",
            "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
            "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
            "WARNING:absl:Found untraced functions such as gru_cell_layer_call_fn, gru_cell_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
          ]
        }
      ],
      "source": [
        "# Guardar el modelo 'one_step_model' en el directorio 'one_step' usando el formato 'SavedModel' de TensorFlow.\n",
        "tf.saved_model.save(one_step_model, 'one_step')\n",
        "\n",
        "# Cargar el modelo previamente guardado desde el directorio 'one_step' y almacenarlo en la variable 'one_step_reloaded'.\n",
        "one_step_reloaded = tf.saved_model.load('one_step')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "_Z9bb_wX6Uuu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc81bcf9-4e14-49ad-9d8f-fcf8a347a563"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "JULIET:\n",
            "I hope here were the case?\n",
            "Your methority, our brother's love, penuse,\n",
            "And ture as strong aside; and so,\n",
            "I hear she bloody in a band, o'\n",
            "Who is lost by, to say I cannot, bear\n",
            "A moider where 'tis awaked with main-maid!\n",
            "Besides, I heard the Duke of Norfolk sinser Marcius;\n",
            "One, one part of wide, and give sick\n",
            "And to be but verited with our field\n",
            "In yet in peace and most abrect\n",
            "Ane seas thrust must action: thy general.\n",
            "\n",
            "FERDINAND:\n",
            "No;\n",
            "I am now--wilt thou mine own likeness. Come hither, Stanley and \n"
          ]
        }
      ],
      "source": [
        "states = None\n",
        "next_char = tf.constant(['JULIET:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(500):\n",
        "  next_char, states = one_step_reloaded.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "print(tf.strings.join(result)[0].numpy().decode(\"utf-8\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y4QwTjAM6A2O"
      },
      "source": [
        "## Avanzado: Personalizar entrenamiento\n",
        "\n",
        "El procedimiento de entrenamiento anterior es simple, pero no brinda mucho control. Se utiliza el método de teacher-forcing, que evita que las predicciones erróneas se retroalimenten al modelo, por lo que el modelo nunca aprende de los errores.\n",
        "\n",
        "Ahora que se ha visto cómo ejecutar el modelo manualmente, a continuación se implementara el bucle de entrenamiento. Esto da un punto de partida si, por ejemplo, se desea implementar _aprendizaje por niveles_ para ayudar a estabilizar la salida en bucle abierto del modelo.\n",
        "\n",
        "La parte más importante de un bucle de entrenamiento personalizado es la función \"paso de entrenamiento\", que se refiere a una iteración donde el modelo procesa un lote de datos y actualiza sus pesos.\n",
        "\n",
        "Se usa `tf.GradientTape` para rastrear los gradientes. Puede aprender más sobre este enfoque leyendo la guía [eager execution guide](https://www.tensorflow.org/guide/eager).\n",
        "\n",
        "El procedimiento básico es:\n",
        "\n",
        "1. Ejecutar el modelo y calcular la pérdida bajo un `tf.GradientTape`.\n",
        "2. Calcular las actualizaciones y aplicarlas al modelo utilizando el optimizador."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "x0pZ101hjwW0"
      },
      "outputs": [],
      "source": [
        "# Se define una clase 'CustomTraining' que hereda de 'MyModel'.\n",
        "class CustomTraining(MyModel):\n",
        "\n",
        "  # Se utiliza el decorador '@tf.function' para convertir la función siguiente en una función TensorFlow (para optimizar su ejecución).\n",
        "  @tf.function\n",
        "  def train_step(self, inputs):\n",
        "\n",
        "      # Se desempaquetan los datos de entrada en variables 'inputs' y 'labels'.\n",
        "      # 'inputs' son los datos que se alimentan al modelo, y 'labels' son las etiquetas verdaderas que se usarán para calcular la pérdida.\n",
        "      inputs, labels = inputs\n",
        "\n",
        "      # Se inicia un contexto 'tf.GradientTape' para registrar operaciones para la diferenciación automática.\n",
        "      with tf.GradientTape() as tape:\n",
        "\n",
        "          # Se obtienen las predicciones del modelo para los datos de entrada.\n",
        "          # El parámetro 'training=True' indica que el modelo está en modo de entrenamiento.\n",
        "          predictions = self(inputs, training=True)\n",
        "\n",
        "          # Se calcula la pérdida entre las etiquetas verdaderas y las predicciones del modelo.\n",
        "          loss = self.loss(labels, predictions)\n",
        "\n",
        "      # Se calculan los gradientes de la pérdida con respecto a las variables entrenables del modelo.\n",
        "      grads = tape.gradient(loss, model.trainable_variables)\n",
        "\n",
        "      # Se aplican los gradientes a las variables entrenables utilizando el optimizador del modelo.\n",
        "      self.optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "      # Se retorna un diccionario con el valor de la pérdida.\n",
        "      return {'loss': loss}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Oc-eJALcK8B"
      },
      "source": [
        "La anterior implementación del método `train_step` sigue las [Convenciones de entrenamiento `train_step` de Keras](https://www.tensorflow.org/guide/keras/customizing_what_happens_in_fit). Esto es opcional, pero permite cambiar el comportamiento del paso de entrenamiento y continuar usando los métodos de keras `Model.compile` y `Model.fit`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "XKyWiZ_Lj7w5"
      },
      "outputs": [],
      "source": [
        "model = CustomTraining(\n",
        "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "U817KUm7knlm"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer = tf.keras.optimizers.Adam(),\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "o694aoBPnEi9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18dccb06-238c-4921-cf7e-8ebee2cf96aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "172/172 [==============================] - 14s 55ms/step - loss: 2.7482\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7e956ae9b670>"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ],
      "source": [
        "model.fit(dataset, epochs=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W8nAtKHVoInR"
      },
      "source": [
        "O, de necesitar más control, puede escribir su propio bucle de entrenamiento personalizado:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d4tSNwymzf-q"
      },
      "outputs": [],
      "source": [
        "# Se establece el número total de épocas para el entrenamiento.\n",
        "EPOCHS = 10\n",
        "\n",
        "# Se crea una métrica para calcular la media de la pérdida.\n",
        "mean = tf.metrics.Mean()\n",
        "\n",
        "# Se inicia el bucle de entrenamiento principal para todas las épocas.\n",
        "for epoch in range(EPOCHS):\n",
        "\n",
        "    # Se registra el tiempo de inicio de la época para calcular cuánto tiempo toma.\n",
        "    start = time.time()\n",
        "\n",
        "    # Se reinician los estados de la métrica 'mean' al inicio de cada época.\n",
        "    mean.reset_states()\n",
        "\n",
        "    # Se itera sobre el conjunto de datos 'dataset', que contiene pares de entrada y objetivo.\n",
        "    for (batch_n, (inp, target)) in enumerate(dataset):\n",
        "\n",
        "        # Se realiza un paso de entrenamiento usando el método 'train_step' del modelo y se obtienen los registros (logs).\n",
        "        logs = model.train_step([inp, target])\n",
        "\n",
        "        # Se actualiza la métrica 'mean' con la pérdida del paso actual.\n",
        "        mean.update_state(logs['loss'])\n",
        "\n",
        "        # Cada 50 lotes, se imprime la pérdida actual.\n",
        "        if batch_n % 50 == 0:\n",
        "            template = f\"Epoch {epoch+1} Batch {batch_n} Loss {logs['loss']:.4f}\"\n",
        "            print(template)\n",
        "\n",
        "    # Cada 5 épocas, se guarda el modelo.\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        model.save_weights(checkpoint_prefix.format(epoch=epoch))\n",
        "\n",
        "    # Al final de cada época, se imprime un resumen con la pérdida media y el tiempo que tomó.\n",
        "    print()\n",
        "    print(f'Epoch {epoch+1} Loss: {mean.result().numpy():.4f}')\n",
        "    print(f'Time taken for 1 epoch {time.time() - start:.2f} sec')\n",
        "    print(\"_\"*80)\n",
        "\n",
        "# Después de completar todas las épocas, se guarda el modelo una vez más.\n",
        "model.save_weights(checkpoint_prefix.format(epoch=epoch))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "text_generation.ipynb",
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}